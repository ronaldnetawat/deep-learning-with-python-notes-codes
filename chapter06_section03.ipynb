{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 6.3 Deploy the model\n",
        "\n",
        "Once our model has successfully cleared the final eval^n on the test set, it's time to deploy the model and begin its productive life!"
      ],
      "metadata": {
        "id": "U1Sba0fQIaen"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3.1 Explain your work to stakeholders and set expectations\n",
        "\n",
        "- Trust is about consistently meeting or exceeding people's expectations.\n",
        "- The actual system is 1/2 of that picture. The other half is setting appropriate expectations before launch.\n",
        "- People not in AI  often have unrealistic expectations from the system. They think it \"understands\" the task.\n",
        "  - To battle this, show them *failure modes* of your model (ex: show what incorrectly classified examples look like).\n",
        "- Often expect human-like performance for tasks that were previously performed by humans. Most ML models do not get there.\n",
        "- Clearly convey the model performance expectations and avoid abstract statements.\n",
        "- Prefer talking about false negative/positive rates.\n",
        "  - Example: “*With these settings, the fraud detection model would have a 5% false negative rate and a 2.5% false positive rate. Every day, an average of 200 valid transactions would be flagged as fraudulent and sent for manual review, and an average of 14 fraudulent transactions would be missed. An average of 266 fraudulent transactions would be correctly caught.*”\n",
        "- Clearly relate model's performance metrics to the business goals.\n",
        "- Make sure to discuss the choice of key launch parameters with the stakeholders. Some parameters cause decisions that involve trade-offs that can only be handled with a deep understanding on the business context.\n",
        "\n"
      ],
      "metadata": {
        "id": "TZCvbIP0Ique"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3.2 Ship an inference model\n",
        "\n",
        "- ML project doesn't end when you save a trained model on Google Colab.\n",
        "- First, we may have to export the model to something other than Python:\n",
        "  - The production environment may not support Python at all (mobile/embedded systems).\n",
        "  - If the rest of the app is not in pyhton, the use of python to serve a model may induce significant overhead.\n",
        "- Since we only want predictions from our production model (a phase called *inference*), we have room to perform various optimizations that can make the model quicker and reduce its memory footprint.\n",
        "\n",
        "Different model deployment options:"
      ],
      "metadata": {
        "id": "1ASjWY9qKsLp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deploying a model as a REST API\n",
        "\n",
        "- A common way.\n",
        "- Could build your own serving app or use *TensorFlow Serving*: TF's own library for shippinh models as APIs. You can deploy a Keras model in minutes with this.\n",
        "- Use this when:\n",
        "  - The app consuming the model's prediction has reliable access to the internet.\n",
        "  - Does not have strict latency requirements.\n",
        "  - Input data sent for reference is not highly sensitive (data will need to be in decrypted form).\n",
        "- Important question to use REST API: wanna host your own code or use a fully-managed 3rd party cloud service like Cloud AI Platform by Google."
      ],
      "metadata": {
        "id": "CO_nAsDXLmK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deploying a model on a device\n",
        "\n",
        "- Sometime we need the model to be on the same device as the app that uses it.\n",
        "- Do this when:\n",
        "  - Strict latency constraints.\n",
        "  - Model can be made sufficiently small. Can use TensorFlow Model Optimization Toolkit.\n",
        "  - Highest possible accuracy is not the critical mission of the task.\n",
        "  - Input data is strictly sensitive.\n",
        "- To deploy a Keras model on a smartphone or an embedded device, use TensorFlow Lite. Runs on Android, iOS, ARM64-based computers, Raspberry Pi, etc. Includes a converter to straightforwardly turn your Keras code into TensorFlow Lite format."
      ],
      "metadata": {
        "id": "inEZmOClNN_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deploying a model in the browser\n",
        "\n",
        "- DL is often used in browser-based or desktop-based JS apps.\n",
        "- Use this when:\n",
        "  - Wanna offload compute to the end user to save your own server costs.\n",
        "  - Input data needs to stay on the end user's device.\n",
        "  - Strict latency constraints.\n",
        "  - Need your app to keep working without connectivity.\n",
        "- Only go with this option if the model is small enough to not hog the CPU, GPU or RAM of the user's device.\n",
        "- Make sure nothing about the model stays confidential (since entire model will be downloaded to the user's device).\n",
        "- Usually possible to recover some info. about the training data so make sure to not make your trained model public if it was trained on sensitive data.\n",
        "- To deploy a model in JS, use TensorFlow.js, a JS library for DL that implements almost all of Keras API and many lower-level TF APIs.\n",
        "- Can easily import a saved Keras model into TF.js to query it as part of your browser-based JS app or desktop Electron app."
      ],
      "metadata": {
        "id": "Wbji5IGpOhTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference model optimization\n",
        "\n",
        "- Optimizing the model for inference is important when the environment it is deployed in has strict constraints on power/memory or for apps with low latency requirements.\n",
        "- 2 popular optimizing techniques:\n",
        "  1. *Weight pruning*: not every weight contributes equally. We can prune the weights that are not important. Reduces memory and compute footprint, at a small cost in performance metrics.\n",
        "  2. *Weight quantization*: DL models are trained with `float32` weights, but you can *quantize* weights to `int8` to get an inference-only model that's 1/4th the size but near the accuracy of the original model. TF ecosystem includes a weight pruning and quantization toolkit: http://tensorflow.org/model_optimization deeply integrated with the Keras API)."
      ],
      "metadata": {
        "id": "GLwl0e7kQV1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3.3 Monitor your model in the wild\n",
        "\n",
        "- Exported inference model, integrated to the app, and dry run it on production data, written unit tests, logged and status-monitored the code. Now deploy the model to production!\n",
        "- This is not the end! Once deployed, gotta monitor its behavior, performance on new dat, interaction with rest of the app, and eventual impact on business metrics.\n",
        "  - *Randomized A/B testing*: send a subset of cases through your new model, and another subset through the old process. After many cases, the difference in outcomes between the 2 is likely attributable to your model.\n",
        "  - Do a regular manual audit on production data. Send some fraction of production data for manual annotation, and compare the model's predictions to new annotations.\n",
        "  - When manual annotations not possible, consider eval^n avenues like user surveys etc."
      ],
      "metadata": {
        "id": "8hZsZZl8RrCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.3.4 Maintain your model\n",
        "\n",
        "- No model lasts forever, remember *concept drift*: characteristics of production data will change over time.\n",
        "- As soon as your model is launched, get ready to train the next generation that will replace it.\n",
        "  - Watch for changes in production data. Are new features available? Should you expand or edit the label set?\n",
        "  - Keep collecting and annotating data, and keep improving your annotation pipeline over time.\n",
        "  - Pay attention to samples that are hard for your model to classify—such samples are most likely to improve performance."
      ],
      "metadata": {
        "id": "5iJpP8PnTHKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "1. New ML project? First define the problem:\n",
        "  - Broader context of problem, end goal and constraints.\n",
        "  - Collect and annotate dataset, understand the data in depth.\n",
        "  - Choose a measure of success. Metrics to monitor validation data.\n",
        "2. Next, develop a model:\n",
        "  - Prepare data.\n",
        "  - Pick evaluation protocol: holdout vali^n, K-fold/iterated K-fold vali^n? Which portion of data to use for validation?\n",
        "  - Achieve statistical power: beat a simple baseline.\n",
        "  - Scale up: overfitting model.\n",
        "  - Regularize model and tune its hyperparams.\n",
        "3. When this is done, it's time for deployment:\n",
        "  - Set appropriate expectations with the stakeholders.\n",
        "  - Optimize final model for inference, ship model to deployment of choice: web server, mobile, browser, embedded device, etc.\n",
        "  - Monitor model's performance in production, keep collecting data to work on the next generation of the model.\n"
      ],
      "metadata": {
        "id": "K42D-4cET87l"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1vNN7Z9fIqYH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}