{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Chapter 6: The universal workflow of machine learning**\n",
        "---\n",
        "- Framing an ML problem.\n",
        "- Developing a working model.\n",
        "- Deploying model in production and maintaining it.\n",
        "\n",
        "Previously:\n",
        "- We already had a labeled dataset, and we could immediatley start training the model.\n",
        "- IRL, this is not the case. We start with a problem, not with a dataset.\n",
        "\n",
        "Universal workflow of ML:\n",
        "1. *Define the task*:\n",
        "  - Understand the problem.\n",
        "  - What customer asked for.\n",
        "  - Collect a dataset.\n",
        "  - Figure out what the data represents.\n",
        "  - Choose how you will measure the success.\n",
        "2. *Develop a model*\n",
        "  - Select a model evaluation protocol.\n",
        "  - Simple baseline to beat.\n",
        "  - Train a first model that has some generalization power, can overfit.\n",
        "  - Then regularize and tune, until you achieve the maximum generalization.\n",
        "3. *Deploy the model*\n",
        "  - Present work to stakeholders.\n",
        "  - Ship the model to a web app, etc.\n",
        "  - Monitor the model's performance, collect data you'll need to create the next-generation model."
      ],
      "metadata": {
        "id": "d_ZKBAiVxgfW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.1 Define the task\n",
        "\n",
        "- Deep understanding of the context of the problem.\n",
        "- Why is your customer trying to solve this problem?\n",
        "- Value they will derive from this solution.\n",
        "- How the model will be used? How it will fit to the customer's business process?\n",
        "- Kind of data available, or could be collected.\n",
        "- Kind of ML task that can be mapped on to the problem.\n"
      ],
      "metadata": {
        "id": "gXLkVTbr7Z1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1.1 Frame the problem\n",
        "\n",
        "Framing involves many detailed meetings/discussions with the stakeholders. Important questions to be answered:\n",
        "1. What will the input data be? What are we trying to predict? Can only predict if training data is available.\n",
        "  - Data availability can be a limiting factor.\n",
        "  - May need to resort to collecting your own data.\n",
        "2. Type of ML task? Binary class^n? Multiclass cass^n? Scalar regression? etc.\n",
        "  - In some cases, ML may not even be the best way to make sense of the data.\n",
        "3. What do existing solutions look like?\n",
        "  - Understand what systems are already in place and how they work!\n",
        "4. Particular constraints you will need to deal with:\n",
        "  - The ins and outs of the deploying of the model.\n",
        "\n",
        "Once done with research, know what your inputs will be, targets will be, and the ML task. You are hypothesizing:\n",
        "1. That your targets can be predicted given your inputs.\n",
        "2. That the data that is available (or that you're going to collect) is sufficiently informative to learn the relationship b/w inputs and targets.\n",
        "\n",
        "Until we have a working model, these are merely hypotheses.\n",
        "\n",
        "Just because we've assembled examples of X inputs and Y targets doesn't mean X contains enough information to predict Y."
      ],
      "metadata": {
        "id": "6saKg_T68Abc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1.2 Collect a dataset\n",
        "\n",
        "- Once task is understood and inputs and targets are know, time to *collect data*: most arduous, expensive and time-consuming part.\n",
        "- Model's ability to generalize comes directly fro the data trained on, # of data points, reliability of labels, quality of features.\n",
        "- If you have extra time, most effective way to allocate is to collect more data rather than spend on model imporvements.\n",
        "- Google researchers' paper \"*The Unreasonable Effectiveness of Data* was first to emphasize the importance of data.\n",
        "If doing supervised learning, after collecting the data, gotta *annotate* them (tage for images, etc.).\n",
        "- Sometimes annotations can be autoatic, sometimes might need to do them ourselves. Labour-heavy process."
      ],
      "metadata": {
        "id": "HObSiv9J_DB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Investing in data annotation infrastructure:\n",
        "\n",
        "- Data annotation determines quality of targets â†’ quality of model. Important questions:\n",
        "1. Should you annotate the data *yourself*?\n",
        "2. Use a *crowdsourcing* platform like Mechanical Turk to collect labels?\n",
        "3. Use services of a specialized data-labeling company?\n",
        "\n",
        "Outsourcing can save money and time, but you lose control. Might be inexpensive but your annotations may end up being noisy.\n",
        "\n",
        "To choose the best option, consider:\n",
        "1. Do data labelers need to be experts or anyone could do it?\n",
        "2. If they need to be experts, could you train people?\n",
        "3. Do YOU understand the way experts come up with the annotations? If not, won't be able to perform manual feature engineering. Might be limiting.\n",
        "\n",
        "If you do decide to label yourself, ask what software you would use. Might need to MAKE YOUR OWN software. This might save a lot of time, so it's wirth investing sometimes.\n"
      ],
      "metadata": {
        "id": "FbzKLf391R5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beware of non-representative data\n",
        "\n",
        "- Models can only make sense of inputs similar to what they were trained on.\n",
        "- Important: training data should be *representative* of the production data.\n",
        "\n",
        "Example:\n",
        "- A model in which users uplaod pictures of their food and model classifies the dish.\n",
        "- You start getting complaints from users.\n",
        "- Reason: the training data was well-lit, professional-quality photos of dishes, while real-life picturesby people are not like that!\n",
        "- *Training model was not representative of the production data*! ML hell!\n",
        "\n",
        "- If possible, collect data from the environment where your model will be used/deployed.\n",
        "- If not possible, make sure you fully understand the differences b/w training and production data, and are actively correcting for those differences.\n",
        "\n",
        "*Concept drift*: occurs when the properties of the production data change over time, causing model accuracy to gradually decay.\n",
        "  - Particularly acute in adversarial contexts like credit card fraud detection (fraud patterns change every day lol).\n",
        "  - It requires constant data collection, and model retraining.\n",
        "\n",
        "Using ML trained on past data to predict future is making the assumption that the future will behave like the past.\n",
        "\n",
        "*Sampling bias*: when your data collection process interacts with what you are trying to predict, resulting in biased measurements. DEWEY DEFEATS TRUMAN is an example of this from real life."
      ],
      "metadata": {
        "id": "eHOEn2GU2lkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1.3 Understand your data\n",
        "\n",
        "- Bad practice to treat the data you're working on as a black box.\n",
        "- Visualize and explore your data to get insight for feature engineering, screen for potential issues, etc.\n",
        "\n",
        "- If data has images/natural language text, take a look at some of your samples directly.\n",
        "- If data has numerical features, plot the histogram of feature values: get idea about range of values, frequency of different values, etc.\n",
        "- If location information, plot on a map. Do clear patterns emerge?\n",
        "- Missing values from some features in some samples? Deal with this too (explained in later section).\n",
        "- Task is classification? Print # of instances of each class in data. Are they roughly equally represented? If not, account for the imbalance.\n",
        "- *Target leaking* check: presence of features in your data that provide info. about the targets and which may not be available in production. Ask: is every feature in your data something that will be available in the same form in production?"
      ],
      "metadata": {
        "id": "Rt-k4A2p9-x3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.1.4 Choose a measure of success\n",
        "\n",
        "- Again, to control something, gotta be able to observe it!\n",
        "- First, define what you mean be success. Accuracy? Precision and recall? Customer retention rate?\n",
        "- Metric guides technical choices you'd make in the project. Should directly align with your higher-level goals.\n",
        "- For balanced classification (every class equally likely), accuracy and area under a *receiver operating characteristic* (ROC) curve (ROC AUC) are common metrics. For imbalanced, precision and recall might be better.\n",
        "- Might need to define your own custom metric as well."
      ],
      "metadata": {
        "id": "3eBnsTXm_DrO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2nGvYmaO2lFv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}