{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 7.4 Writing your own training and validation loops\n",
        "\n",
        "- `fit()` gives a nice balance between ease of use and flexibility.\n",
        "- But this built-in workflow is solely focused on *supervised learning*, we might encounter *generative learning*, *self-supervised learning* or *reinforcement learning* and we might need to write our own custom training logic.\n",
        "\n",
        "Contents of a typical training loop:\n",
        "1. Run the forward pass inside a gradient tape to get loss value.\n",
        "2. Get gradients of loss w.r.t. weights.\n",
        "3. Update weights as to lower the loss.\n",
        "\n",
        "This is essentially what `fit()` does under the hood.\n",
        "\n",
        "Let's learn to implement `fit()` from scratch to learn how to write any training algorithm we may need."
      ],
      "metadata": {
        "id": "QNn3c2Jpo8rM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.4.1 Training versus inference\n",
        "\n",
        "In training loops that we've seen so far:\n",
        "* Step 1 (forward pass) is done via `predictions = model(inputs)`\n",
        "* Step 2 (gradients retrieval) is done via `gradients = tape.gradient(loss, model.weights)`\n",
        "\n",
        "Some Keras layers expose a `training` Boolean argument in their `call()` method, so do Functional and Sequential models.\n",
        "\n",
        "* Remember to pass `training=True` when aclling a Keras model during the forward pass.\n",
        "* In retrieving gradients, remember to use `model.trainable_weights` instead of `model.weights` in `GradientTape`.\n",
        "\n",
        "> Some weights are non-trainable which are meant to be updated during forward pass.\n",
        "\n",
        "So, a supervised-learning training step must look like this:\n",
        "\n",
        "```python\n",
        "def train_step(inputs, targets):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(inputs, training=True)\n",
        "    loss = loss_fn(targets, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, model.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(model.trainable_weight, gradients))\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "cwowYHDLrYDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.4.2 Low-level usage of metrics\n",
        "\n",
        "- For low-level training loops (loops where you have manual control over the training process), we might wanna leverage Keras metrics.\n",
        "- Metrics API: simply call `update_state(y_true, y_pred)` for each batch of targets and preds and use `result()` to query current metric value:"
      ],
      "metadata": {
        "id": "cRWHc7lHuSkV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0srlr1jo50w",
        "outputId": "b40cb2bf-9cba-44d8-9ec4-53e0d98d433f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result: 1.00\n"
          ]
        }
      ],
      "source": [
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "\n",
        "metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "targets = [0, 1, 2]\n",
        "predictions = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
        "metric.update_state(targets, predictions)\n",
        "current_result = metric.result()\n",
        "print(f\"result: {current_result:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tracking average of scalar value like the model's loss. Do this via `keras.metrics.Mean` metric:"
      ],
      "metadata": {
        "id": "H8HX1U5HwIaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "values = [0, 1, 2, 3, 4]\n",
        "mean_tracker = keras.metrics.Mean()\n",
        "for value in values:\n",
        "  mean_tracker.update_state(value)\n",
        "print(f\"mean of values: {mean_tracker.result():.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhshCVcvva8E",
        "outputId": "8d49f85c-a08e-4e4d-d004-1e3ab1d145fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean of values: 2.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember to use `metric.reset_state()` to reset current results (at start of training epoch or start of evaluation)."
      ],
      "metadata": {
        "id": "tPvWPMNNwiCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.4.3 A complete training and evaluation loop\n",
        "\n",
        "\n",
        "### Writing a step-by-step training loop: the training step function"
      ],
      "metadata": {
        "id": "Z7p7SsRmwrru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "def get_mnist_model():\n",
        "    inputs = keras.Input(shape=(28 * 28,))\n",
        "    features = layers.Dense(512, activation=\"relu\")(inputs)\n",
        "    features = layers.Dropout(0.5)(features)\n",
        "    outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "U62U0fvawca8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf = tensorflow\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "zZikOj5cySBv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_mnist_model()\n",
        "\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy() # prepare the loss func\n",
        "optimizer = keras.optimizers.RMSprop() # prepare the optimizer\n",
        "metrics = [keras.metrics.SparseCategoricalAccuracy()] # list of metrics to monitor\n",
        "loss_tracking_metric = keras.metrics.Mean() # mean metric to keep track of loss average\n",
        "\n",
        "def train_step(inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True) # forward pass. Note: training=True\n",
        "        loss = loss_fn(targets, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_weights) # back pass. Note: trainable_weights\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_weights)) # back pass. Note: trainable_weights\n",
        "\n",
        "    # keep track of the metrics\n",
        "    logs = {}\n",
        "    for metric in metrics:\n",
        "        metric.update_state(targets, predictions)\n",
        "        logs[metric.name] = metric.result()\n",
        "\n",
        "    # keep track of the loss average\n",
        "    loss_tracking_metric.update_state(loss)\n",
        "    logs[\"loss\"] = loss_tracking_metric.result()\n",
        "    return logs   # return the current values of the metrics and the loss"
      ],
      "metadata": {
        "id": "rORPO7PKxV8-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gotta reset the state of our metrics before the start of each epoch and before running the evaluation. Write a utility function for that.\n",
        "\n",
        "### Resetting the metrics"
      ],
      "metadata": {
        "id": "LQ9Tf24Wy25G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_metrics():\n",
        "  for metric in metrics:\n",
        "    metric.reset_state()\n",
        "  loss_tracking_metric.reset_state()"
      ],
      "metadata": {
        "id": "Af1p_jmbysZB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's lay out the complete training loop.\n",
        "\n",
        "### The loop itself"
      ],
      "metadata": {
        "id": "udU8Ko2pzdXH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's load the data first\n",
        "(images, labels), (test_images, test_labels) = mnist.load_data()\n",
        "images = images.reshape((60000, 28 * 28)).astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28)).astype(\"float32\") / 255\n",
        "train_images, val_images = images[10000:], images[:10000]\n",
        "train_labels, val_labels = labels[10000:], labels[:10000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0Am2Q4kz3oT",
        "outputId": "0d7ac0b6-efc6-4c58-aac0-4a194a7a611a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this object turns NumPy data into an iterator that iterates over the data in batches of size 32\n",
        "training_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "training_dataset = training_dataset.batch(32)\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    reset_metrics()\n",
        "    for inputs_batch, targets_batch in training_dataset:\n",
        "        logs = train_step(inputs_batch, targets_batch)\n",
        "    print(f\"Results at the end of epoch {epoch}\")\n",
        "    for key, value in logs.items():\n",
        "        print(f\"...{key}: {value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KN3sY5N-zMU8",
        "outputId": "074350d7-c1f9-454c-b7dc-49c59591b263"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results at the end of epoch 0\n",
            "...sparse_categorical_accuracy: 0.9152\n",
            "...loss: 0.2894\n",
            "Results at the end of epoch 1\n",
            "...sparse_categorical_accuracy: 0.9534\n",
            "...loss: 0.1602\n",
            "Results at the end of epoch 2\n",
            "...sparse_categorical_accuracy: 0.9636\n",
            "...loss: 0.1317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now let's write the evaluation loop step-by-step"
      ],
      "metadata": {
        "id": "p6irdCGV0ot-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_step() is just train_step() without the updating steps\n",
        "def test_step(inputs, targets):\n",
        "    predictions = model(inputs, training=False) # note that training=False here\n",
        "    loss = loss_fn(targets, predictions)\n",
        "\n",
        "    logs = {}\n",
        "    for metric in metrics:\n",
        "        metric.update_state(targets, predictions)\n",
        "        logs[\"val_\" + metric.name] = metric.result()\n",
        "\n",
        "    loss_tracking_metric.update_state(loss)\n",
        "    logs[\"val_loss\"] = loss_tracking_metric.result()\n",
        "    return logs\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
        "val_dataset = val_dataset.batch(32)\n",
        "reset_metrics()\n",
        "for inputs_batch, targets_batch in val_dataset:\n",
        "    logs = test_step(inputs_batch, targets_batch)\n",
        "print(\"evaluation results:\")\n",
        "for key, value in logs.items():\n",
        "    print(f\"...{key}: {value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-zDAc75zzFU",
        "outputId": "ce1db901-ae5d-491a-e4e6-493ed47854aa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluation results:\n",
            "...val_sparse_categorical_accuracy: 0.9680\n",
            "...val_loss: 0.1168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at some more features supported by `fit()` and `evaluate()`, including large-scale distributed computation, performance optimizations. Let's look at one of these optimizations: TensorFlow function compilation."
      ],
      "metadata": {
        "id": "8OjSywk61H8O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.4.4 Make it fast with tf.function\n",
        "\n",
        "- Our custom loops are running pretty slower than the built-in methods.\n",
        "- Because, by default, TF code is executed line by line, *eagerly*, just like regular Python code.\n",
        "- It's good from debugging POV but not from a performance POV.\n",
        "- It's more performant to *compile* your TF code into a *computation graph* that can be globally optimized that line-by-line code cannot.\n",
        "- Just add a `@tf.function` decorator to do this.\n",
        "\n",
        "### `@tf.function` decorator to our evaluation-step function"
      ],
      "metadata": {
        "id": "ia9pq20h1Xu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function # the only line that changed\n",
        "def test_step(inputs, targets):\n",
        "    predictions = model(inputs, training=False) # note that training=False here\n",
        "    loss = loss_fn(targets, predictions)\n",
        "\n",
        "    logs = {}\n",
        "    for metric in metrics:\n",
        "        metric.update_state(targets, predictions)\n",
        "        logs[\"val_\" + metric.name] = metric.result()\n",
        "\n",
        "    loss_tracking_metric.update_state(loss)\n",
        "    logs[\"val_loss\"] = loss_tracking_metric.result()\n",
        "    return logs\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
        "val_dataset = val_dataset.batch(32)\n",
        "reset_metrics()\n",
        "for inputs_batch, targets_batch in val_dataset:\n",
        "    logs = test_step(inputs_batch, targets_batch)\n",
        "print(\"evaluation results:\")\n",
        "for key, value in logs.items():\n",
        "    print(f\"...{key}: {value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTqwSqHU1Cht",
        "outputId": "6822409d-6271-41f8-af33-b37afdda9702"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "evaluation results:\n",
            "...val_sparse_categorical_accuracy: 0.9680\n",
            "...val_loss: 0.1168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This took less than half of the previous method!\n",
        "\n",
        "> When debugging, run it eagerly, but once you know the code is working, add `@tf.function`."
      ],
      "metadata": {
        "id": "Z1C1-poy2tAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.4.5 Leveraging fit() with a custom training loop\n",
        "\n",
        "- Writing our own training loop (that we've been doing previously) gives us flexibility but we need to write a lot of code and miss out on features.\n",
        "- There is a common ground. Providing a custom training step function and let the framework do the rest.\n",
        "\n",
        "* Create a new class that subclasses the `keras.Model`\n",
        "* Override the `train_step(self, data)`\n",
        "* Implement a `metrics` property that tracks model's `Metrics` instances. Enables model to automatically call `reset_state()` on metrics at start of a call to `evaluate()`, so we don't have to do this by hand."
      ],
      "metadata": {
        "id": "f48X60j528kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#these metric object will be used to track the average of per-batch losses during training and eval\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "class CustomModel(keras.Model):\n",
        "    def train_step(self, data): # overriding the train_step method\n",
        "        inputs, targets = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(inputs, training=True) # using self(inputs, training=True) instead of model(inputs, training=True) since our model is the class itself\n",
        "            loss = loss_fn(targets, predictions)\n",
        "        gradients = tape.gradient(loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
        "\n",
        "        loss_tracker.update_state(loss) #update the loss tracker metric that tracks the average of the loss\n",
        "        return {\"loss\": loss_tracker.result()} # return the average loss so far\n",
        "\n",
        "    # any metric to be reset across epochs should be listed here\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [loss_tracker]"
      ],
      "metadata": {
        "id": "UHK_ldmN2rb2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's instantiate our custom model\n",
        "\n",
        "inputs = keras.Input(shape=(28 * 28,))\n",
        "features = layers.Dense(512, activation=\"relu\")(inputs)\n",
        "features = layers.Dropout(0.5)(features)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "model = CustomModel(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.RMSprop()) # loss is already defined outside the model\n",
        "model.fit(train_images, train_labels, epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhXmcywD5EOq",
        "outputId": "414e3693-f695-4a69-bae5-272816112a4a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.4548\n",
            "Epoch 2/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - loss: 0.1608\n",
            "Epoch 3/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - loss: 0.1320\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b081f3cb390>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Can do this whether building Sequential, Functional or subclass models.\n",
        "- Don't need to use `@tf.function` when you override `train_setp`, the framework does it for us.\n",
        "\n",
        "After calling `compile()`, we get access to:\n",
        "- `self.compiled_loss`: loss function passed to compile.\n",
        "- `self.compiled_metrics`: wrapper for list of metrics we passed, allows to call `self.compiled_metrics.update_state()` to update all metrics at once.\n",
        "- `self.metrics`: actual list of metrics passed to `compile()`.\n",
        "\n",
        "Thus:"
      ],
      "metadata": {
        "id": "kK1Zr2jF5RtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomModel(keras.Model):\n",
        "    def train_step(self, data):\n",
        "        inputs, targets = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(inputs, training=True)\n",
        "            loss = self.compiled_loss(targets, predictions) # compute loss via self.compiled_loss\n",
        "\n",
        "        gradients = tape.gradient(loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
        "        self.compiled_metrics.update_state(targets, predictions) #update model's metrics via self.compiled_metrics\n",
        "        return {m.name: m.result() for m in self.metrics} #return a dict mapping metric names to their current value"
      ],
      "metadata": {
        "id": "CYYUqyk44diY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# try it:\n",
        "inputs = keras.Input(shape=(28 * 28,))\n",
        "features = layers.Dense(512, activation=\"relu\")(inputs)\n",
        "features = layers.Dropout(0.5)(features)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "model = CustomModel(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
        "model.fit(train_images, train_labels, epochs=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPM3-WGB6M3d",
        "outputId": "916c355c-fb5c-4115-f7fb-ef7f408d802b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py:617: UserWarning: `model.compiled_loss()` is deprecated. Instead, use `model.compute_loss(x, y, y_pred, sample_weight, training)`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py:592: UserWarning: `model.compiled_metrics()` is deprecated. Instead, use e.g.:\n",
            "```\n",
            "for metric in self.metrics:\n",
            "    metric.update_state(y, y_pred)\n",
            "```\n",
            "\n",
            "  return self._compiled_metrics_update_state(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - sparse_categorical_accuracy: 0.8654 - loss: 0.1000\n",
            "Epoch 2/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - sparse_categorical_accuracy: 0.9526 - loss: 0.1000\n",
            "Epoch 3/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - sparse_categorical_accuracy: 0.9606 - loss: 0.1000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b080fbeae50>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary:\n",
        "\n",
        "- Keras offers a spectrum of different workflows. *Progressive disclosure of complexity*. All interoperate together.\n",
        "- Can build models using `Sequential`, Functional API, or subclassing `Model` class.\n",
        "- Simplest way to train and evaluate model is via the default `fit()` and `evaluate()` methods.\n",
        "- Callbacks are a way to monitor models during call to `fit()` and automatically take actions based on the state of the model.\n",
        "- Can take full control of `fit()` by overriding `train_step()` method.\n",
        "- Can write our own training loops entirely from scratch. Useful for implementing brand-new algorithms."
      ],
      "metadata": {
        "id": "EyxMbqHF6gU1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Obg_NaJ_6ekm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}